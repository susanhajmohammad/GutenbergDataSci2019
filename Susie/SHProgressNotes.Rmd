---
title: "SH Progress Notes"
author: "Your name(s)"
date: "Spring 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Week 6

Goals:

  long term:
  
  - Find the names mentioned close to each other in each novel
  - Find the adjectives closest to certain character names
  - THis would include looking for how to code to select the nearest 5 words around a character's name for example.  
  - FIrst figure out how to split the book into lines 
  - also look into pronouns because the character's name isn't always mentioned in the text.  This could show us a difference in mood when referring to women vs. men (she/her vs he/him).  
  
  SHort term: 
  - update the metadata
     do this by manually putting in the missing gothic books into a csv an rbind it to the df I have already made.  
  - the data frame should include all the info but ultimately be split up by chapter per row.  
  
  
Time: 



Summary:

Completed Intermediate R



Notes:

#### Intermediate R

how to use grepl(), grep(), sub(), and gsub() functions.  Grepl returns true/false, grep returns the number of which item matched in the list.  

```{r}

# The emails vector has already been defined for you
emails <- c("john.doe@ivyleague.edu", "education@world.gov", "dalai.lama@peace.org",
            "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

# Use grepl() to match for "edu"

grepl(pattern = "edu", x = emails)

# Use grep() to match for "edu", save result to hits

hits <- grep(pattern = "edu", emails)

# Subset emails using hits

emails[hits]



```


the pattern that matches for an ".edu" email address:

```{r}
# The emails vector has already been defined for you
emails <- c("john.doe@ivyleague.edu", "education@world.gov", "dalai.lama@peace.org",
            "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

# Use grepl() to match for .edu addresses more robustly

grepl(pattern = "@.*\\.edu$", x = emails)

# Use grep() to match for .edu addresses more robustly, save result to hits

hits <- grep(pattern = "@.*\\.edu$", x = emails )

# Subset emails using hits

emails[hits]
```


The function sub() replaces a pattern in a string with what you tell it to, using "replacement":

```{r}
# The emails vector has already been defined for you
emails <- c("john.doe@ivyleague.edu", "education@world.gov", "global@peace.org",
            "invalid.edu", "quant@bigdatacollege.edu", "cookie.monster@sesame.tv")

# Use sub() to convert the email domains to datacamp.edu

sub(pattern = "@.*\\.edu$", replacement = "@datacamp.edu", x = emails)


```

When entering dates, you have to enter it as "year-month-day".

you can change this by saying format = ...


```{r}
# Get the current date: today

today <- Sys.Date()

# See what today looks like under the hood

unclass(today)

# Get the current time: now

now <- Sys.time()

# See what now looks like under the hood

unclass(now)

```



```{r}
# Definition of character strings representing dates
str1 <- "May 23, '96"
str2 <- "2012-03-15"
str3 <- "30/January/2006"

# Convert the strings to dates: date1, date2, date3
date1 <- as.Date(str1, format = "%b %d, '%y")

date2 <- as.Date(str2)

date3 <- as.Date(str3, format = "%d/%B/%Y")
```


A list of the codes for formatting : 

dates:

%Y: 4-digit year (1982)
%y: 2-digit year (82)
%m: 2-digit month (01)
%d: 2-digit day of the month (13)
%A: weekday (Wednesday)
%a: abbreviated weekday (Wed)
%B: month (January)
%b: abbreviated month (Jan)

Times:

%H: hours as a decimal number (00-23)
%I: hours as a decimal number (01-12)
%M: minutes as a decimal number
%S: seconds as a decimal number
%T: shorthand notation for the typical format %H:%M:%S
%p: AM/PM indicator

If you want to look it up in R, ?strptime 

```{r}
# day1, day2, day3, day4 and day5 are already available in the workspace

day5 - day1


# Difference between last and first pizza day


# Create vector pizza
pizza <- c(day1, day2, day3, day4, day5)

# Create differences between consecutive pizza days: day_diff

day_diff <- diff(pizza)
day_diff
# Average period between two consecutive pizza days

mean(day_diff)

```



```{r}
# Convert astro to vector of Date objects: astro_dates

astro_dates <- as.Date(astro, format = "%d-%b-%Y")

# Convert meteo to vector of Date objects: meteo_dates

meteo_dates <- as.Date(meteo, format = "%B %d, %y")

# Calculate the maximum absolute difference between astro_dates and meteo_dates

max(abs(astro_dates - meteo_dates))
```



#### Text mining 


when text mining: 

1. you have to define your problem or what you are looking for specifically

2. Identify what text you are collecting (medium, data integrity)

3. Organize the text (maybe by like title or author or chapter)

4. Feature extraction (This could be sentiment analysis, or extracting word "tokens" into different "matrices")

5. Perform some analysis

6. Reach an insight (answer your question or at least have a conlusion)




Semantic Parsing vs. Bag of words


Semantic Parsing

We care about word type and order

This breaks up sentences into phrases (noun and verb) then tags them ("named entity", "verb", "article", "noun")

This is useful because it tells you about many features of each word. 




Bag of words


doesnt care about word type or order

Words are just attributes of the text. 



## How to split a text into sentences using strsplit function:

```{r}



unlist(strsplit(string, "(?<=\\.)\\s(?=[A-Z])", perl = T))



```




## How to split a text into sentences using tokenize_sentences() function:

```{r}

library(tokenizers)
tokenize_sentences(x)


```







